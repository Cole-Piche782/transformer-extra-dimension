# transformer-extra-dimension
This is my transformer neural network that makes use of a new type of positional encoding I came up with, and seems to perform better than transformers that use sinusoidal positional encoding for translating Portuguese to English. It only runs on linux or wsl since it needs certain libraries that don't exist for windows. It also outputs some error messages after it finishes running sometimes. That's alright because the purpose of this project is not to demonstrate flawless programming, but instead to simply demonstrate my ability to come up with modifications to neural network structures that improve them, and then program using AI libraries well enough to implement my ideas and therefore demonstrate their feasibility.
My form of positional encoding simply adds an extra dimension to the word embedding to store a single value indicating the position of the word, rather than adding sinusoidal or linear embedding values to the existing dimensions.

I took much of the original code for a basic transformer neural network from this tutorial: https://www.tensorflow.org/text/tutorials/transformer but the code copied as-is from there didn't work and I had to make one or two modifications to get it not to crash. I also used chatgpt to help me figure out which Tensorflow and Keras methods to use in order to add my new dimension to hold the positional encoding. I did this because I suspected that if I had just used a large number of loops to add this dimension that it would cause a drastic performance decrease, and I suspected python libraries like this would have more elegant solution available where I can add the dimension using just one or two function calls, and searching for the exact method to use for this without the help of ChatGPT would have been quite time consuming.
