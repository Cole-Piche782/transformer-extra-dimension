# transformer-extra-dimension
This is my transformer neural network that makes use of a new type of positional encoding I came up with, and seems to perform better than transformers that use sinusoidal positional encoding for translating Portuguese to English. It only runs on linux or wsl since it needs certain libraries that don't exist for windows. It also outputs some error messages after it finishes running sometimes. That's alright because the purpose of this project is not to demonstrate flawless programming, but instead to simply demonstrate my ability to come up with modifications to neural network structures that improve them, and then program using AI libraries well enough to implement a prototype to demonstrates the feasilbility of my proposed modification.
My form of positional encoding simply adds an extra dimension to the word embedding to store a single value indicating the position of the word, rather than adding sinusoidal or linear embedding values to the existing dimensions.
